{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eNQEwiA9Chrp",
        "B4KwgEGpCliU",
        "q0Jbx80UCsRr",
        "tyxZQvyJC0ot",
        "GC1zBx2IDUiL",
        "KEPyCIJdBrr9",
        "A_qYF3qQByR1",
        "RLnyfGjkB2Qn",
        "MnIuuLClCKPw",
        "k3YWtJslGzp6",
        "y9Kup8XDrW55"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNQEwiA9Chrp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo88VsKe89NZ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Input\n",
        "from keras.layers import Masking\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GRU\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRSYwaI9NNP"
      },
      "source": [
        "from urllib import request\n",
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "import scipy.sparse \n",
        "import gc\n",
        "import itertools\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4KwgEGpCliU"
      },
      "source": [
        "\n",
        "\n",
        "# Dataset download and encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udwVKBiQBXRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2aae466-12ed-4019-af0b-523f8f98b41c"
      },
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
        "\n",
        "def download_dataset(download_path, url):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(\"Downloading dataset...\")\n",
        "        request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path, extract_path):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(download_path, \"r\") as loaded_zip:\n",
        "        loaded_zip.extractall(extract_path)\n",
        "    print(\"Extraction completed!\")\n",
        "\n",
        "\n",
        "download_dataset(dataset_path, url)\n",
        "extract_dataset(dataset_path, dataset_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Download complete!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaSwE8MoKdq3"
      },
      "source": [
        "def encode_dataset(dataset_folder, range, test=False): \n",
        "    special_characters = string.punctuation\n",
        "    dataframe_rows = []\n",
        "\n",
        "    for filename in sorted(os.listdir(dataset_folder))[range[0]: range[1]]:\n",
        "      \n",
        "      file_path = os.path.join(dataset_folder, filename)\n",
        "      \n",
        "      with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "        sentence = []\n",
        "        tags = []\n",
        "        for line in text_file:\n",
        "          \n",
        "          try:  \n",
        "            text, tag, _ = line.split()\n",
        "            sentence.append(text)\n",
        "            tags.append(tag)\n",
        "\n",
        "          except ValueError:\n",
        "            \n",
        "            if not test:\n",
        "              dataframe_row = {\n",
        "                \"Sentence\": sentence,\n",
        "                \"Tags\": tags\n",
        "              }\n",
        "            \n",
        "              sentence = []\n",
        "              tags = []\n",
        "              dataframe_rows.append(dataframe_row)\n",
        "      \n",
        "        if test:\n",
        "          dataframe_rows.append({\"Document\": sentence, \"Tags\": tags})\n",
        "  \n",
        "    print(\"Dataset encoded!\")\n",
        "    if test:\n",
        "      return pd.DataFrame(dataframe_rows, columns=[\"Document\", \"Tags\"])\n",
        "    else:\n",
        "      return pd.DataFrame(dataframe_rows, columns=[\"Sentence\", \"Tags\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGGWCPPCoDiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06b9cc7-42bd-4443-cd09-fb01bfbfc32a"
      },
      "source": [
        "treebank_folder = os.path.join(dataset_folder, \"dependency_treebank\")\n",
        "\n",
        "train_df = encode_dataset(treebank_folder, range=(0, 100))\n",
        "print(\"Train set: {}\".format(train_df.shape))\n",
        "\n",
        "val_df = encode_dataset(treebank_folder, range=(100, 150))\n",
        "print(\"Val set: {}\".format(val_df.shape))\n",
        "\n",
        "test_df = encode_dataset(treebank_folder, range=(150, 200), test=True)\n",
        "print(\"Test set: {}\".format(test_df.shape))\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset encoded!\n",
            "Train set: (1863, 2)\n",
            "Dataset encoded!\n",
            "Val set: (1249, 2)\n",
            "Dataset encoded!\n",
            "Test set: (49, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Lorillard, Inc., ,, the, unit, of, New, York-...</td>\n",
              "      <td>[NNP, NNP, ,, DT, NN, IN, JJ, JJ, NNP, NNP, WD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Although, preliminary, findings, were, report...</td>\n",
              "      <td>[IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence                                               Tags\n",
              "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...\n",
              "1  [A, form, of, asbestos, once, used, to, make, ...  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...\n",
              "2  [The, asbestos, fiber, ,, crocidolite, ,, is, ...  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...\n",
              "3  [Lorillard, Inc., ,, the, unit, of, New, York-...  [NNP, NNP, ,, DT, NN, IN, JJ, JJ, NNP, NNP, WD...\n",
              "4  [Although, preliminary, findings, were, report...  [IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Jbx80UCsRr"
      },
      "source": [
        "# Vocabulary, co-occurrence and embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1AqgvDOvqmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e56cc76-b160-44da-877f-aad7e77694a2"
      },
      "source": [
        "embedding_dimension = 300\n",
        "download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "try:\n",
        "  embedding_model = gloader.load(download_path)\n",
        "except ValueError as e:\n",
        "  print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "  print(\"Glove: 50, 100, 200, 300\")\n",
        "  raise e\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mai4G3q0KkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbd8273-045b-47f2-9324-b171d7aa8539"
      },
      "source": [
        "def build_vocabulary(corpus):\n",
        "\n",
        "    wordlist = []\n",
        "    for x in corpus:\n",
        "      wordlist.extend(x)\n",
        "    words = set(wordlist)\n",
        "    word_vocab = {}\n",
        "    inverse_word_vocab = {}\n",
        "    for i, word in enumerate(words):\n",
        "      word_vocab[i] = word\n",
        "      inverse_word_vocab[word] = i\n",
        "    \n",
        "    return word_vocab, inverse_word_vocab, words\n",
        "\n",
        "\n",
        "train_idx_to_word, train_word_to_idx, train_word_listing = build_vocabulary(train_df['Sentence'])\n",
        "val_idx_to_word, val_word_to_idx, val_word_listing = build_vocabulary(val_df['Sentence'])\n",
        "test_idx_to_word, test_word_to_idx, test_word_listing = build_vocabulary(test_df['Document'])\n",
        "\n",
        "print(\"{} words in training set\".format(len(train_word_listing)))\n",
        "print(\"{} words in validation set\".format(len(val_word_listing)))\n",
        "print(\"{} words in test set\".format(len(test_word_listing)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7841 words in training set\n",
            "5768 words in validation set\n",
            "3623 words in test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMUsfvc-kccR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d79d01-d9af-4251-cff3-34f1d2025c0e"
      },
      "source": [
        "def co_occurrence_count(corpus, idx_to_word, word_to_idx, window_size=1):\n",
        "\n",
        "    data = []\n",
        "    index_i = []\n",
        "    index_j = []\n",
        "\n",
        "    for _, words in corpus.iteritems():\n",
        "      for j, word in enumerate(words[::]):\n",
        "        start = max(0, j-window_size)\n",
        "        end = min(len(words), j + window_size+1)\n",
        "        sub_sentence = words[start:end]\n",
        "        for w in sub_sentence:\n",
        "          if word != w:\n",
        "            data.append(1.)\n",
        "            index_i.append(word_to_idx[word])\n",
        "            index_j.append(word_to_idx[w])\n",
        "            \n",
        "    co_occurrence = scipy.sparse.csr_matrix((data, (index_i, index_j)))\n",
        "\n",
        "    return co_occurrence\n",
        "\n",
        "window_size = 1\n",
        "\n",
        "# Clean RAM before re-running this code snippet to avoid session crash\n",
        "if 'train_co_occurrence_matrix' in globals():\n",
        "    del train_co_occurrence_matrix\n",
        "    gc.collect()\n",
        "    time.sleep(10.)\n",
        "if 'val_co_occurrence_matrix' in globals():\n",
        "    del val_co_occurrence_matrix\n",
        "    gc.collect()\n",
        "    time.sleep(10.)\n",
        "\n",
        "\n",
        "print(\"Building co-occurrence count matrix... (it may take a while...)\")\n",
        "train_co_occurrence_matrix = co_occurrence_count(train_df['Sentence'], train_idx_to_word, train_word_to_idx, window_size)\n",
        "val_co_occurrence_matrix = co_occurrence_count(val_df['Sentence'], val_idx_to_word, val_word_to_idx, window_size)\n",
        "test_co_occurrence_matrix = co_occurrence_count(test_df['Document'], test_idx_to_word, test_word_to_idx, window_size)\n",
        "\n",
        "print(\"Building completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building co-occurrence count matrix... (it may take a while...)\n",
            "Building completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsUbO77O1ZJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d333075-6d83-49e6-9bb5-19f289c17401"
      },
      "source": [
        "def check_OOV_terms(embedding_model, word_listing):\n",
        "\n",
        "    OOV = [word for word in word_listing if word not in embedding_model.vocab]\n",
        "    return OOV\n",
        "\n",
        "\n",
        "train_oov_terms = check_OOV_terms(embedding_model, train_word_listing)\n",
        "val_oov_terms = check_OOV_terms(embedding_model, val_word_listing)\n",
        "test_oov_terms = check_OOV_terms(embedding_model, test_word_listing)\n",
        "\n",
        "print(\"Total OOV terms in train: {0} ({1:.2f}%)\".format(len(train_oov_terms), float(len(train_oov_terms)) / len(train_word_listing)*100))\n",
        "print(\"Total OOV terms in val: {0} ({1:.2f}%)\".format(len(val_oov_terms), float(len(val_oov_terms)) / len(val_word_listing)*100))\n",
        "print(\"Total OOV terms in test: {0} ({1:.2f}%)\".format(len(test_oov_terms), float(len(test_oov_terms)) / len(test_word_listing)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms in train: 2281 (29.09%)\n",
            "Total OOV terms in val: 1479 (25.64%)\n",
            "Total OOV terms in test: 957 (26.41%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWOJfcHm3WK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f60570-1dbe-49b6-9e7c-e70820041711"
      },
      "source": [
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, co_occurrence_matrix):\n",
        "   \n",
        "    embedding_matrix = np.ndarray((len(word_to_idx), embedding_dimension))\n",
        "\n",
        "    for w in word_to_idx:\n",
        "      if w in embedding_model.vocab:\n",
        "        embedding_matrix[word_to_idx[w], :] = embedding_model.get_vector(w)\n",
        "      else:\n",
        "        occurrences = co_occurrence_matrix[word_to_idx[w]]\n",
        "\n",
        "        close_words = []\n",
        "        for i in occurrences.indices:\n",
        "          if idx_to_word[i] in embedding_model.vocab:\n",
        "            close_words.append(embedding_model.get_vector(idx_to_word[i]))\n",
        "        if len(close_words) == 0:\n",
        "          embedding_matrix[word_to_idx[w], :] = np.random.rand(1, embedding_dimension)\n",
        "        else:\n",
        "          embedding_matrix[word_to_idx[w], :] = np.average(close_words)\n",
        "\n",
        "    return embedding_matrix \n",
        "  \n",
        "train_embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, train_word_to_idx, train_idx_to_word, train_co_occurrence_matrix)\n",
        "val_embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, val_word_to_idx, val_idx_to_word, val_co_occurrence_matrix)\n",
        "test_embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, test_word_to_idx, test_idx_to_word, test_co_occurrence_matrix)\n",
        "\n",
        "print(\"Train embedding matrix shape: {}\".format(train_embedding_matrix.shape))\n",
        "print(\"Val embedding matrix shape: {}\".format(val_embedding_matrix.shape))\n",
        "print(\"Test embedding matrix shape: {}\".format(test_embedding_matrix.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train embedding matrix shape: (7841, 300)\n",
            "Val embedding matrix shape: (5768, 300)\n",
            "Test embedding matrix shape: (3623, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyxZQvyJC0ot"
      },
      "source": [
        "# Embedding sentences and tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0w_OK8g6mSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2babc81-8471-4c6d-aa27-1600fff8ed5a"
      },
      "source": [
        "def embed_sentence(embedding_matrix, sentence, word_to_idx):\n",
        "  embedded = []\n",
        "  for w in sentence:\n",
        "    embedded.append(embedding_matrix[word_to_idx[w]])\n",
        "  return embedded\n",
        "\n",
        "def embed_tags(df, tags_dict):\n",
        "  df['Embedded tags indexes'] = [list(map(tags_dict.get, tags)) for tags in df.Tags]\n",
        "  df['Embedded tags one hot'] = [to_categorical(tags, num_classes=len(tags_dict)+1, dtype='int32') for tags in df['Embedded tags indexes']]\n",
        "  \n",
        "\n",
        "unique_tags = set(itertools.chain.from_iterable(pd.concat([train_df, val_df]).Tags))\n",
        "tags_dict = {item:val+1 for val,item in enumerate(unique_tags)}\n",
        "\n",
        "train_df['Embedded sentence'] = [embed_sentence(train_embedding_matrix, sentence, train_word_to_idx) for sentence in train_df['Sentence']]\n",
        "val_df['Embedded sentence'] = [embed_sentence(val_embedding_matrix, sentence, val_word_to_idx) for sentence in val_df['Sentence']]\n",
        "test_df['Embedded sentence'] = [embed_sentence(test_embedding_matrix, document, test_word_to_idx) for document in test_df['Document']]\n",
        "\n",
        "embed_tags(train_df, tags_dict)\n",
        "embed_tags(val_df, tags_dict)\n",
        "embed_tags(test_df, tags_dict)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Embedded sentence</th>\n",
              "      <th>Embedded tags indexes</th>\n",
              "      <th>Embedded tags one hot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "      <td>[[0.1550200958180733, 0.48581725412669174, 0.3...</td>\n",
              "      <td>[9, 9, 22, 24, 1, 16, 22, 7, 11, 40, 28, 33, 4...</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "      <td>[[0.000534959661308676, 0.000534959661308676, ...</td>\n",
              "      <td>[40, 28, 33, 28, 41, 45, 3, 11, 9, 28, 1, 17, ...</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "      <td>[[-0.006698744371533394, -0.006698744371533394...</td>\n",
              "      <td>[40, 28, 28, 22, 28, 22, 17, 41, 16, 33, 5, 17...</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Lorillard, Inc., ,, the, unit, of, New, York-...</td>\n",
              "      <td>[NNP, NNP, ,, DT, NN, IN, JJ, JJ, NNP, NNP, WD...</td>\n",
              "      <td>[[-0.0017415573820471764, -0.00174155738204717...</td>\n",
              "      <td>[9, 9, 22, 40, 28, 33, 16, 16, 9, 9, 42, 17, 9...</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Although, preliminary, findings, were, report...</td>\n",
              "      <td>[IN, JJ, NNS, VBD, VBN, RBR, IN, DT, NN, IN, ,...</td>\n",
              "      <td>[[-0.002929060021415353, -0.002929060021415353...</td>\n",
              "      <td>[33, 16, 1, 39, 45, 37, 33, 40, 28, 33, 22, 40...</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence  ...                              Embedded tags one hot\n",
              "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...  ...  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...\n",
              "1  [A, form, of, asbestos, once, used, to, make, ...  ...  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "2  [The, asbestos, fiber, ,, crocidolite, ,, is, ...  ...  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "3  [Lorillard, Inc., ,, the, unit, of, New, York-...  ...  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...\n",
              "4  [Although, preliminary, findings, were, report...  ...  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1zBx2IDUiL"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9l9bBHEr-I_"
      },
      "source": [
        "def valid_labels(tags_dict):\n",
        "\n",
        "  labels = []\n",
        "  target_names = []\n",
        "\n",
        "  for key, value in tags_dict.items():\n",
        "    if not key in string.punctuation and key != \"``\" and key != \"''\":\n",
        "      labels.append(value)\n",
        "      target_names.append(key)\n",
        "\n",
        "  return [labels, target_names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lpBXxu_zQ8x"
      },
      "source": [
        "TRAIN_MAX_SENTENCE_LENGTH = train_df.Sentence.str.len().max()\n",
        "VAL_MAX_SENTENCE_LENGTH = val_df.Sentence.str.len().max()\n",
        "TEST_MAX_DOCUMENT_LENGTH = test_df.Document.str.len().max()\n",
        "\n",
        "MAX_SENTENCE_LENGTH = max(TRAIN_MAX_SENTENCE_LENGTH, VAL_MAX_SENTENCE_LENGTH)\n",
        "\n",
        "EMBEDDING_SIZE = embedding_dimension\n",
        "\n",
        "NUM_CLASSES = len(tags_dict) + 1\n",
        "\n",
        "[VALID_LABELS, VALID_LABELS_NAMES] = valid_labels(tags_dict)\n",
        "\n",
        "input_shape = (None, EMBEDDING_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyOChFf_ApM6"
      },
      "source": [
        "X_train = train_df['Embedded sentence'].values\n",
        "y_train = train_df['Embedded tags one hot'].values\n",
        "\n",
        "X_val = val_df['Embedded sentence'].values\n",
        "y_val = val_df['Embedded tags one hot'].values\n",
        "\n",
        "X_train_padded = pad_sequences(X_train, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\", dtype='float32')\n",
        "y_train_padded = pad_sequences(y_train, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "X_val_padded = pad_sequences(X_val, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\", dtype='float32')\n",
        "y_val_padded = pad_sequences(y_val, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "callback = EarlyStopping(monitor='val_acc', mode='max', patience=5, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEPyCIJdBrr9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# BILSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9yH1XURH-oM"
      },
      "source": [
        "def BiLSTM(NUM_CLASSES, input_shape):\n",
        "  \n",
        "  lstm_model = Sequential(name=\"BiLSTM\")\n",
        "\n",
        "  lstm_model.add(Masking(mask_value=0, input_shape=input_shape))\n",
        "  lstm_model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "  lstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\")))\n",
        "  lstm_model.add(Dropout(0.2))\n",
        "  \n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  lstm_model.compile(loss      =  'categorical_crossentropy',\n",
        "                    optimizer =  adam,\n",
        "                    metrics   =  ['acc'])\n",
        "  \n",
        "  lstm_model.summary()\n",
        "  \n",
        "  return lstm_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as5PIj2aKzdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e616acf3-94e2-47b6-8a56-961d13a70841"
      },
      "source": [
        "biLSTM = BiLSTM(NUM_CLASSES, input_shape)\n",
        "biLSTM_history = biLSTM.fit(X_train_padded, y_train_padded, batch_size=128, epochs=50, validation_data=(X_val_padded, y_val_padded), callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BiLSTM\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking (Masking)            (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 1024)        3330048   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 46)          47150     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 46)          0         \n",
            "=================================================================\n",
            "Total params: 3,377,198\n",
            "Trainable params: 3,377,198\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 3s 232ms/step - loss: 0.4763 - acc: 0.3844 - val_loss: 0.0990 - val_acc: 0.7234\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 2s 102ms/step - loss: 0.3718 - acc: 0.6495 - val_loss: 0.0598 - val_acc: 0.8297\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 2s 102ms/step - loss: 0.3465 - acc: 0.7086 - val_loss: 0.0465 - val_acc: 0.8644\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 1s 100ms/step - loss: 0.3418 - acc: 0.7307 - val_loss: 0.0399 - val_acc: 0.8822\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 1s 98ms/step - loss: 0.3306 - acc: 0.7501 - val_loss: 0.0369 - val_acc: 0.8921\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 1s 100ms/step - loss: 0.3319 - acc: 0.7604 - val_loss: 0.0350 - val_acc: 0.8958\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 1s 99ms/step - loss: 0.3237 - acc: 0.7740 - val_loss: 0.0356 - val_acc: 0.8958\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 2s 100ms/step - loss: 0.3198 - acc: 0.7817 - val_loss: 0.0355 - val_acc: 0.8954\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 2s 101ms/step - loss: 0.3204 - acc: 0.7852 - val_loss: 0.0362 - val_acc: 0.8979\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 1s 99ms/step - loss: 0.3184 - acc: 0.7907 - val_loss: 0.0369 - val_acc: 0.8975\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 1s 99ms/step - loss: 0.3160 - acc: 0.7944 - val_loss: 0.0377 - val_acc: 0.8994\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 2s 101ms/step - loss: 0.3156 - acc: 0.7965 - val_loss: 0.0398 - val_acc: 0.8967\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 1s 99ms/step - loss: 0.3191 - acc: 0.7957 - val_loss: 0.0397 - val_acc: 0.8982\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 1s 100ms/step - loss: 0.3196 - acc: 0.7959 - val_loss: 0.0407 - val_acc: 0.8993\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 2s 101ms/step - loss: 0.3145 - acc: 0.7994 - val_loss: 0.0418 - val_acc: 0.8970\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 1s 99ms/step - loss: 0.3134 - acc: 0.8005 - val_loss: 0.0420 - val_acc: 0.8975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_qYF3qQByR1"
      },
      "source": [
        "# BIGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPJKgyicw0fw"
      },
      "source": [
        "def BiGRU(NUM_CLASSES, input_shape):\n",
        "  gru_model = Sequential(name=\"BiGRU\")\n",
        "\n",
        "  gru_model.add(Masking(mask_value=0, input_shape=input_shape))\n",
        "  gru_model.add(Bidirectional(GRU(512, return_sequences=True)))\n",
        "  gru_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\")))\n",
        "  gru_model.add(Dropout(0.2))\n",
        "  \n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  gru_model.compile(loss      =  'categorical_crossentropy',\n",
        "                    optimizer =  adam,\n",
        "                    metrics   =  ['acc'])\n",
        "  \n",
        "  gru_model.summary()\n",
        "  \n",
        "  return gru_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O9hcXWoIBeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba22a13-d049-4182-98b1-d75fd62501e5"
      },
      "source": [
        "biGRU = BiGRU(NUM_CLASSES, input_shape)\n",
        "biGRU_history = biGRU.fit(X_train_padded, y_train_padded, batch_size=128, epochs=50, validation_data=(X_val_padded, y_val_padded), callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BiGRU\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_1 (Masking)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 1024)        2500608   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, None, 46)          47150     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 46)          0         \n",
            "=================================================================\n",
            "Total params: 2,547,758\n",
            "Trainable params: 2,547,758\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 3s 221ms/step - loss: 0.4457 - acc: 0.4631 - val_loss: 0.0765 - val_acc: 0.7827\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3513 - acc: 0.6867 - val_loss: 0.0522 - val_acc: 0.8509\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3379 - acc: 0.7254 - val_loss: 0.0435 - val_acc: 0.8737\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3326 - acc: 0.7454 - val_loss: 0.0402 - val_acc: 0.8807\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3310 - acc: 0.7581 - val_loss: 0.0378 - val_acc: 0.8886\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3274 - acc: 0.7698 - val_loss: 0.0360 - val_acc: 0.8942\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3210 - acc: 0.7819 - val_loss: 0.0368 - val_acc: 0.8917\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 1s 89ms/step - loss: 0.3197 - acc: 0.7879 - val_loss: 0.0369 - val_acc: 0.8962\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 0.3176 - acc: 0.7925 - val_loss: 0.0373 - val_acc: 0.8976\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3151 - acc: 0.7974 - val_loss: 0.0395 - val_acc: 0.8984\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3152 - acc: 0.7985 - val_loss: 0.0400 - val_acc: 0.8964\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3156 - acc: 0.7984 - val_loss: 0.0408 - val_acc: 0.8989\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3148 - acc: 0.7995 - val_loss: 0.0421 - val_acc: 0.9000\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 0.3152 - acc: 0.7993 - val_loss: 0.0433 - val_acc: 0.8992\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3154 - acc: 0.7993 - val_loss: 0.0437 - val_acc: 0.8991\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3134 - acc: 0.8006 - val_loss: 0.0442 - val_acc: 0.9002\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3148 - acc: 0.7997 - val_loss: 0.0440 - val_acc: 0.8993\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 1s 91ms/step - loss: 0.3136 - acc: 0.8007 - val_loss: 0.0451 - val_acc: 0.9001\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3166 - acc: 0.7988 - val_loss: 0.0452 - val_acc: 0.8994\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 1s 90ms/step - loss: 0.3151 - acc: 0.7997 - val_loss: 0.0461 - val_acc: 0.8996\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 1s 92ms/step - loss: 0.3130 - acc: 0.8010 - val_loss: 0.0459 - val_acc: 0.8993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLnyfGjkB2Qn"
      },
      "source": [
        "# BIBILSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOg3yhGAMDYP"
      },
      "source": [
        "def BibiLSTM(NUM_CLASSES, input_shape):\n",
        "  \n",
        "  lstm_model = Sequential(name=\"BibiLSTM\")\n",
        "\n",
        "  lstm_model.add(Masking(mask_value=0, input_shape=input_shape))\n",
        "  lstm_model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "  lstm_model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "  lstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\")))\n",
        "  lstm_model.add(Dropout(0.2))\n",
        "  \n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  lstm_model.compile(loss      =  'categorical_crossentropy',\n",
        "                    optimizer =  adam,\n",
        "                    metrics   =  ['acc'])\n",
        "  \n",
        "  lstm_model.summary()\n",
        "  \n",
        "  return lstm_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnj2ZryHMZe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34dbdb9-e18b-4c5c-df2d-ecdb84072122"
      },
      "source": [
        "bibiLSTM = BibiLSTM(NUM_CLASSES, input_shape)\n",
        "bibiLSTM_history = bibiLSTM.fit(X_train_padded, y_train_padded, batch_size=128, epochs=50, validation_data=(X_val_padded, y_val_padded), callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BibiLSTM\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_2 (Masking)          (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 1024)        3330048   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, None, 1024)        6295552   \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, None, 46)          47150     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 46)          0         \n",
            "=================================================================\n",
            "Total params: 9,672,750\n",
            "Trainable params: 9,672,750\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 6s 433ms/step - loss: 0.5981 - acc: 0.1189 - val_loss: 0.2816 - val_acc: 0.1832\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 3s 192ms/step - loss: 0.5151 - acc: 0.1778 - val_loss: 0.2706 - val_acc: 0.2154\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 3s 191ms/step - loss: 0.4988 - acc: 0.2473 - val_loss: 0.2361 - val_acc: 0.3793\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 3s 188ms/step - loss: 0.4673 - acc: 0.3598 - val_loss: 0.1838 - val_acc: 0.4552\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 3s 191ms/step - loss: 0.4316 - acc: 0.4500 - val_loss: 0.1418 - val_acc: 0.5818\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.4045 - acc: 0.5382 - val_loss: 0.1151 - val_acc: 0.6780\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3863 - acc: 0.5984 - val_loss: 0.0919 - val_acc: 0.7442\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 3s 195ms/step - loss: 0.3745 - acc: 0.6396 - val_loss: 0.0801 - val_acc: 0.7815\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 3s 192ms/step - loss: 0.3642 - acc: 0.6662 - val_loss: 0.0700 - val_acc: 0.8102\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 3s 192ms/step - loss: 0.3552 - acc: 0.6867 - val_loss: 0.0642 - val_acc: 0.8225\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3490 - acc: 0.7001 - val_loss: 0.0586 - val_acc: 0.8358\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3453 - acc: 0.7106 - val_loss: 0.0532 - val_acc: 0.8508\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 3s 189ms/step - loss: 0.3402 - acc: 0.7208 - val_loss: 0.0515 - val_acc: 0.8521\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3388 - acc: 0.7268 - val_loss: 0.0493 - val_acc: 0.8612\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3396 - acc: 0.7318 - val_loss: 0.0469 - val_acc: 0.8666\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 3s 190ms/step - loss: 0.3359 - acc: 0.7387 - val_loss: 0.0454 - val_acc: 0.8697\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3396 - acc: 0.7396 - val_loss: 0.0432 - val_acc: 0.8770\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3370 - acc: 0.7456 - val_loss: 0.0426 - val_acc: 0.8791\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 3s 190ms/step - loss: 0.3378 - acc: 0.7466 - val_loss: 0.0422 - val_acc: 0.8790\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3371 - acc: 0.7458 - val_loss: 0.0407 - val_acc: 0.8858\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3290 - acc: 0.7576 - val_loss: 0.0411 - val_acc: 0.8837\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3296 - acc: 0.7584 - val_loss: 0.0389 - val_acc: 0.8895\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.3283 - acc: 0.7608 - val_loss: 0.0483 - val_acc: 0.8646\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.3279 - acc: 0.7602 - val_loss: 0.0395 - val_acc: 0.8881\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 3s 191ms/step - loss: 0.3316 - acc: 0.7646 - val_loss: 0.0388 - val_acc: 0.8903\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 3s 195ms/step - loss: 0.3255 - acc: 0.7690 - val_loss: 0.0384 - val_acc: 0.8923\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 3s 195ms/step - loss: 0.3243 - acc: 0.7713 - val_loss: 0.0396 - val_acc: 0.8897\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 3s 197ms/step - loss: 0.3262 - acc: 0.7717 - val_loss: 0.0382 - val_acc: 0.8935\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.3234 - acc: 0.7750 - val_loss: 0.0389 - val_acc: 0.8931\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 3s 197ms/step - loss: 0.3222 - acc: 0.7761 - val_loss: 0.0385 - val_acc: 0.8937\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 3s 195ms/step - loss: 0.3191 - acc: 0.7794 - val_loss: 0.0397 - val_acc: 0.8925\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 3s 193ms/step - loss: 0.3216 - acc: 0.7789 - val_loss: 0.0393 - val_acc: 0.8946\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 3s 196ms/step - loss: 0.3223 - acc: 0.7799 - val_loss: 0.0398 - val_acc: 0.8958\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 3s 192ms/step - loss: 0.3189 - acc: 0.7830 - val_loss: 0.0412 - val_acc: 0.8885\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3208 - acc: 0.7821 - val_loss: 0.0401 - val_acc: 0.8952\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 3s 191ms/step - loss: 0.3201 - acc: 0.7834 - val_loss: 0.0425 - val_acc: 0.8891\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 3s 194ms/step - loss: 0.3169 - acc: 0.7865 - val_loss: 0.0434 - val_acc: 0.8908\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 3s 196ms/step - loss: 0.3216 - acc: 0.7819 - val_loss: 0.0422 - val_acc: 0.8936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnIuuLClCKPw"
      },
      "source": [
        "# BILSTM + CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oaEDAMvcyKT"
      },
      "source": [
        "!pip install tf2crf -q\n",
        "from tf2crf import CRF, ModelWithCRFLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94tDFEllgaqc"
      },
      "source": [
        "def LSTMCRF(NUM_CLASSES, input_shape):\n",
        "  \n",
        "  input = Input(shape=(input_shape))\n",
        "  mask = Masking(mask_value=0)(input)\n",
        "  bilstm = Bidirectional(LSTM(512, return_sequences=True))(mask)\n",
        "  dense = Dense(NUM_CLASSES, activation=None)(bilstm)\n",
        "  drop = Dropout(0.2)(dense)\n",
        "  crf = CRF()\n",
        "  output = crf(drop)\n",
        "\n",
        "  base_model = Model(input, output)\n",
        "  \n",
        "  lstmcrf_model = ModelWithCRFLoss(base_model)\n",
        "  \n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  lstmcrf_model.compile(optimizer = adam, metrics=['acc'])\n",
        "  \n",
        "  return lstmcrf_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oicHOzI-iHv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b01baa-9d98-4e01-967b-ad223ab03f26"
      },
      "source": [
        "y_train_indexes = train_df['Embedded tags indexes']\n",
        "y_val_indexes = val_df['Embedded tags indexes']\n",
        "\n",
        "y_train_indexes_padded = pad_sequences(y_train_indexes, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\", value=0)\n",
        "y_val_indexes_padded = pad_sequences(y_val_indexes, maxlen=MAX_SENTENCE_LENGTH, padding=\"post\", value=0)\n",
        "\n",
        "callback = EarlyStopping(monitor='val_val_accuracy', mode='max', patience=5, restore_best_weights=True)\n",
        "\n",
        "lstmcrf = LSTMCRF(NUM_CLASSES, input_shape=input_shape)\n",
        "lstmcrf_history = lstmcrf.fit(X_train_padded, y_train_indexes_padded, batch_size=128, epochs=50, validation_data=(X_val_padded, y_val_indexes_padded), callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 12s 821ms/step - crf_loss: 64.8017 - accuracy: 0.3864 - val_crf_loss_val: 24.8295 - val_val_accuracy: 0.7256\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 10s 667ms/step - crf_loss: 28.4405 - accuracy: 0.6757 - val_crf_loss_val: 14.5361 - val_val_accuracy: 0.8360\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 10s 668ms/step - crf_loss: 20.9714 - accuracy: 0.7611 - val_crf_loss_val: 11.1177 - val_val_accuracy: 0.8725\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 10s 658ms/step - crf_loss: 17.1164 - accuracy: 0.8071 - val_crf_loss_val: 9.5659 - val_val_accuracy: 0.8895\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 10s 667ms/step - crf_loss: 14.6131 - accuracy: 0.8425 - val_crf_loss_val: 8.8172 - val_val_accuracy: 0.8988\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 10s 665ms/step - crf_loss: 12.4038 - accuracy: 0.8652 - val_crf_loss_val: 8.7878 - val_val_accuracy: 0.9004\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 10s 666ms/step - crf_loss: 11.2041 - accuracy: 0.8818 - val_crf_loss_val: 8.3376 - val_val_accuracy: 0.9049\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 10s 661ms/step - crf_loss: 9.4687 - accuracy: 0.8981 - val_crf_loss_val: 8.4933 - val_val_accuracy: 0.9033\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 10s 659ms/step - crf_loss: 8.7020 - accuracy: 0.9077 - val_crf_loss_val: 8.2704 - val_val_accuracy: 0.9078\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 10s 661ms/step - crf_loss: 7.7643 - accuracy: 0.9166 - val_crf_loss_val: 9.3103 - val_val_accuracy: 0.9061\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 10s 654ms/step - crf_loss: 7.1499 - accuracy: 0.9237 - val_crf_loss_val: 9.4160 - val_val_accuracy: 0.9043\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 10s 655ms/step - crf_loss: 6.4774 - accuracy: 0.9300 - val_crf_loss_val: 9.8566 - val_val_accuracy: 0.9048\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 10s 662ms/step - crf_loss: 6.1580 - accuracy: 0.9331 - val_crf_loss_val: 10.2848 - val_val_accuracy: 0.9051\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 10s 666ms/step - crf_loss: 5.5986 - accuracy: 0.9368 - val_crf_loss_val: 10.5015 - val_val_accuracy: 0.9058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN7DekKQoEWm"
      },
      "source": [
        "# BIBILSTM + CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPhzxZQ9oDmT"
      },
      "source": [
        "def BibiLSTMCRF(NUM_CLASSES, input_shape):\n",
        "  \n",
        "  input = Input(shape=(input_shape))\n",
        "  mask = Masking(mask_value=0)(input)\n",
        "  bilstm = Bidirectional(LSTM(512, return_sequences=True))(mask)\n",
        "  bilstm = Bidirectional(LSTM(512, return_sequences=True))(bilstm)\n",
        "  dense = TimeDistributed(Dense(NUM_CLASSES, activation=None))(bilstm)\n",
        "  drop = Dropout(0.2)(dense)\n",
        "  crf = CRF()\n",
        "  output = crf(drop)\n",
        "\n",
        "  base_model = Model(input, output)\n",
        "  \n",
        "  lstmcrf_model = ModelWithCRFLoss(base_model)\n",
        "  \n",
        "  adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  lstmcrf_model.compile(optimizer = adam, metrics=['acc'])\n",
        "  \n",
        "  return lstmcrf_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtpBAVE6oOpk",
        "outputId": "42e41502-9509-4e1f-ab35-263319e39aa7"
      },
      "source": [
        "bibilstmcrf = BibiLSTMCRF(NUM_CLASSES, input_shape=input_shape)\n",
        "bibilstmcrf_history = bibilstmcrf.fit(X_train_padded, y_train_indexes_padded, batch_size=128, epochs=50, validation_data=(X_val_padded, y_val_indexes_padded), callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 15s 1s/step - crf_loss: 103.0247 - accuracy: 0.1271 - val_crf_loss_val: 70.2889 - val_val_accuracy: 0.2388\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 11s 739ms/step - crf_loss: 65.4237 - accuracy: 0.2618 - val_crf_loss_val: 56.4111 - val_val_accuracy: 0.3380\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 11s 756ms/step - crf_loss: 54.3039 - accuracy: 0.3609 - val_crf_loss_val: 43.9892 - val_val_accuracy: 0.4900\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 11s 749ms/step - crf_loss: 43.2283 - accuracy: 0.4942 - val_crf_loss_val: 32.3164 - val_val_accuracy: 0.6638\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 11s 745ms/step - crf_loss: 33.2831 - accuracy: 0.6093 - val_crf_loss_val: 23.1448 - val_val_accuracy: 0.7622\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 11s 747ms/step - crf_loss: 26.8499 - accuracy: 0.6873 - val_crf_loss_val: 18.3536 - val_val_accuracy: 0.7989\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 11s 742ms/step - crf_loss: 22.9576 - accuracy: 0.7347 - val_crf_loss_val: 15.5445 - val_val_accuracy: 0.8272\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 11s 748ms/step - crf_loss: 19.7526 - accuracy: 0.7727 - val_crf_loss_val: 13.7376 - val_val_accuracy: 0.8463\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 11s 746ms/step - crf_loss: 17.7730 - accuracy: 0.7970 - val_crf_loss_val: 12.1676 - val_val_accuracy: 0.8638\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 11s 748ms/step - crf_loss: 16.1296 - accuracy: 0.8154 - val_crf_loss_val: 10.9706 - val_val_accuracy: 0.8788\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 11s 752ms/step - crf_loss: 14.7581 - accuracy: 0.8345 - val_crf_loss_val: 10.6062 - val_val_accuracy: 0.8834\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 11s 745ms/step - crf_loss: 13.3890 - accuracy: 0.8474 - val_crf_loss_val: 9.7012 - val_val_accuracy: 0.8902\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 11s 749ms/step - crf_loss: 12.4484 - accuracy: 0.8597 - val_crf_loss_val: 9.6200 - val_val_accuracy: 0.8891\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 11s 743ms/step - crf_loss: 12.0210 - accuracy: 0.8673 - val_crf_loss_val: 9.5396 - val_val_accuracy: 0.8899\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 11s 750ms/step - crf_loss: 10.8660 - accuracy: 0.8770 - val_crf_loss_val: 8.5758 - val_val_accuracy: 0.9019\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 11s 751ms/step - crf_loss: 10.1862 - accuracy: 0.8838 - val_crf_loss_val: 8.3321 - val_val_accuracy: 0.9034\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 11s 752ms/step - crf_loss: 9.6774 - accuracy: 0.8900 - val_crf_loss_val: 9.5378 - val_val_accuracy: 0.8915\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 11s 754ms/step - crf_loss: 9.2632 - accuracy: 0.8935 - val_crf_loss_val: 8.5552 - val_val_accuracy: 0.8998\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 11s 751ms/step - crf_loss: 8.8786 - accuracy: 0.8971 - val_crf_loss_val: 8.2516 - val_val_accuracy: 0.9048\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 11s 750ms/step - crf_loss: 8.1741 - accuracy: 0.9063 - val_crf_loss_val: 8.3157 - val_val_accuracy: 0.9035\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 11s 751ms/step - crf_loss: 7.8672 - accuracy: 0.9073 - val_crf_loss_val: 8.9459 - val_val_accuracy: 0.9042\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 11s 751ms/step - crf_loss: 7.5896 - accuracy: 0.9110 - val_crf_loss_val: 8.6061 - val_val_accuracy: 0.9055\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 11s 750ms/step - crf_loss: 7.3667 - accuracy: 0.9135 - val_crf_loss_val: 10.2330 - val_val_accuracy: 0.8925\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 11s 755ms/step - crf_loss: 7.4247 - accuracy: 0.9091 - val_crf_loss_val: 9.0324 - val_val_accuracy: 0.9029\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 11s 751ms/step - crf_loss: 6.9882 - accuracy: 0.9153 - val_crf_loss_val: 9.3523 - val_val_accuracy: 0.9053\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 11s 746ms/step - crf_loss: 6.6426 - accuracy: 0.9204 - val_crf_loss_val: 8.9870 - val_val_accuracy: 0.9060\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 11s 746ms/step - crf_loss: 6.3743 - accuracy: 0.9211 - val_crf_loss_val: 9.4851 - val_val_accuracy: 0.9044\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 11s 756ms/step - crf_loss: 6.0976 - accuracy: 0.9227 - val_crf_loss_val: 10.0671 - val_val_accuracy: 0.9034\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 11s 754ms/step - crf_loss: 5.9949 - accuracy: 0.9248 - val_crf_loss_val: 9.5235 - val_val_accuracy: 0.9070\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 11s 743ms/step - crf_loss: 5.8450 - accuracy: 0.9260 - val_crf_loss_val: 10.2691 - val_val_accuracy: 0.9067\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 11s 754ms/step - crf_loss: 5.6432 - accuracy: 0.9299 - val_crf_loss_val: 10.0636 - val_val_accuracy: 0.9066\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 11s 744ms/step - crf_loss: 5.4596 - accuracy: 0.9299 - val_crf_loss_val: 10.1952 - val_val_accuracy: 0.9035\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 11s 747ms/step - crf_loss: 5.4850 - accuracy: 0.9299 - val_crf_loss_val: 10.6391 - val_val_accuracy: 0.9034\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 11s 752ms/step - crf_loss: 5.3418 - accuracy: 0.9324 - val_crf_loss_val: 10.5436 - val_val_accuracy: 0.9016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXDGd7fCF8Q"
      },
      "source": [
        "# Evaluation of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXQPDsGNHeRG"
      },
      "source": [
        "def evaluate_model(y_true, y_pred, crf=False):\n",
        "\n",
        "  y_true_valid = []\n",
        "  y_pred_valid = []\n",
        "\n",
        "  for i in range(y_pred.shape[0]):\n",
        "    for j in range(y_pred.shape[1]):\n",
        "      if crf:\n",
        "        if y_true[i][j] != 0:\n",
        "          y_true_valid.append(y_true[i][j])\n",
        "          y_pred_valid.append(y_pred[i][j])\n",
        "      else:\n",
        "        if not np.all((y_true[i][j] == 0)):\n",
        "          y_true_valid.append(np.argmax(y_true[i][j]))\n",
        "          y_pred_valid.append(np.argmax(y_pred[i][j]))\n",
        "  \n",
        "\n",
        "  print(\"\\t\\tEvaluating punctuation\\t\\tNot evaluating punctuation\")\n",
        "  \n",
        "  averages = ['macro', 'micro', 'weighted']\n",
        "  \n",
        "  for avg in averages:\n",
        "    print(\"F1 {}:\\t\\t {:.2f}  \\t\\t\\t\\t{:.2f}\".format(avg, f1_score(y_true_valid, y_pred_valid, average=avg, zero_division=0),\n",
        "                                              f1_score(y_true_valid, y_pred_valid, labels=VALID_LABELS, average=avg, zero_division=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MHDT9PqXVnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab17e9a-c6cc-4ff6-ec31-859f426585ff"
      },
      "source": [
        "models = [biLSTM, biGRU, bibiLSTM]\n",
        "models_histories = [biLSTM_history, biGRU_history, bibiLSTM_history]\n",
        "\n",
        "models_crf = [lstmcrf, bibilstmcrf]\n",
        "models_crf_histories =[lstmcrf_history, bibilstmcrf_history]\n",
        "\n",
        "for i,h in enumerate(models_histories):\n",
        "  m = models[i]\n",
        "  print(\"-\" * 80)\n",
        "  print(\"\\t\\t\\tModel evaluation: \" + m.name +\"\\n\")\n",
        "  print(\"Train accuracy: {:.2f}\".format(max(h.history['acc'])))\n",
        "  print(\"Validation accuracy: {:.2f}\\n\".format(max(h.history['val_acc'])))\n",
        "  pred = m.predict(X_val_padded)\n",
        "  evaluate_model(y_val_padded, pred)\n",
        "\n",
        "for i,h in enumerate(models_crf_histories):\n",
        "  m = models_crf[i]\n",
        "  print(\"-\" * 80)\n",
        "  print(\"\\t\\t\\tModel evaluation: \" + \"Bi\"*(i+1) +\"LSTMCRF\\n\")\n",
        "  print(\"Train accuracy: {:.2f}\".format(max(h.history['accuracy'])))\n",
        "  print(\"Validation accuracy: {:.2f}\\n\".format(max(h.history['val_val_accuracy'])))\n",
        "  pred = m.predict(X_val_padded)\n",
        "  evaluate_model(y_val_indexes_padded, pred[0], crf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "\t\t\tModel evaluation: BiLSTM\n",
            "\n",
            "Train accuracy: 0.80\n",
            "Validation accuracy: 0.90\n",
            "\n",
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.75  \t\t\t\t0.71\n",
            "F1 micro:\t\t 0.90  \t\t\t\t0.89\n",
            "F1 weighted:\t\t 0.90  \t\t\t\t0.88\n",
            "--------------------------------------------------------------------------------\n",
            "\t\t\tModel evaluation: BiGRU\n",
            "\n",
            "Train accuracy: 0.80\n",
            "Validation accuracy: 0.90\n",
            "\n",
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.74  \t\t\t\t0.72\n",
            "F1 micro:\t\t 0.90  \t\t\t\t0.89\n",
            "F1 weighted:\t\t 0.90  \t\t\t\t0.89\n",
            "--------------------------------------------------------------------------------\n",
            "\t\t\tModel evaluation: BibiLSTM\n",
            "\n",
            "Train accuracy: 0.79\n",
            "Validation accuracy: 0.90\n",
            "\n",
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.71  \t\t\t\t0.66\n",
            "F1 micro:\t\t 0.90  \t\t\t\t0.88\n",
            "F1 weighted:\t\t 0.89  \t\t\t\t0.88\n",
            "--------------------------------------------------------------------------------\n",
            "\t\t\tModel evaluation: BiLSTMCRF\n",
            "\n",
            "Train accuracy: 0.94\n",
            "Validation accuracy: 0.91\n",
            "\n",
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.77  \t\t\t\t0.73\n",
            "F1 micro:\t\t 0.91  \t\t\t\t0.90\n",
            "F1 weighted:\t\t 0.91  \t\t\t\t0.89\n",
            "--------------------------------------------------------------------------------\n",
            "\t\t\tModel evaluation: BiBiLSTMCRF\n",
            "\n",
            "Train accuracy: 0.93\n",
            "Validation accuracy: 0.91\n",
            "\n",
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.75  \t\t\t\t0.71\n",
            "F1 micro:\t\t 0.91  \t\t\t\t0.90\n",
            "F1 weighted:\t\t 0.90  \t\t\t\t0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YWtJslGzp6"
      },
      "source": [
        "# Testing best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAOjbVN1G-cj"
      },
      "source": [
        "X_test = test_df['Embedded sentence'].values\n",
        "y_test = test_df['Embedded tags indexes'].values\n",
        "X_test_padded = pad_sequences(X_test, maxlen=TEST_MAX_DOCUMENT_LENGTH, padding=\"post\", value=0, dtype=\"float32\")\n",
        "y_test_indexes_padded = pad_sequences(y_test, maxlen=TEST_MAX_DOCUMENT_LENGTH, padding=\"post\", value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6khn08mG-4l"
      },
      "source": [
        "y_test_pred = lstmcrf.predict(X_test_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J2hy7iqHAoH",
        "outputId": "fe1bbf73-376d-4c04-b649-432d59238c12"
      },
      "source": [
        "evaluate_model(y_test_indexes_padded, y_test_pred[0], crf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\tEvaluating punctuation\t\tNot evaluating punctuation\n",
            "F1 macro:\t\t 0.82  \t\t\t\t0.70\n",
            "F1 micro:\t\t 0.91  \t\t\t\t0.89\n",
            "F1 weighted:\t\t 0.91  \t\t\t\t0.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Kup8XDrW55"
      },
      "source": [
        "# Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-cQDawIsNCV"
      },
      "source": [
        "y_true_valid = y_test_indexes_padded[y_test_indexes_padded != 0]\n",
        "y_pred_valid = y_test_pred[0][y_test_indexes_padded != 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld94YO7QrcbP",
        "outputId": "85cde890-a23e-4d19-a718-92c30e5c4c41"
      },
      "source": [
        "print(classification_report(y_true_valid, y_pred_valid, labels=VALID_LABELS, target_names=VALID_LABELS_NAMES, zero_division=0))\n",
        "_, _, f1, sup = precision_recall_fscore_support(y_true_valid, y_pred_valid, labels=VALID_LABELS, zero_division=0)\n",
        "\n",
        "worst = np.argsort(f1)[:6]\n",
        "w_tags = []\n",
        "for w in worst:\n",
        "  w_tags.append(VALID_LABELS_NAMES[w])\n",
        "\n",
        "print(\"Tags with worst f1 value\")\n",
        "print(\"\\tTags:\\t\\t\\t\", w_tags)\n",
        "print(\"\\t F1:\\t\\t\\t\", f1[worst])\n",
        "print(\"       Support:\\t\\t\\t\", sup[worst])\n",
        "print(\"Support percentage on total(%):\\t\", np.around(sup[worst] / sum(sup) * 100, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          MD       0.98      1.00      0.99       167\n",
            "          CD       0.96      0.96      0.96       858\n",
            "          JJ       0.72      0.78      0.75       918\n",
            "         RBS       1.00      0.33      0.50         3\n",
            "          UH       0.00      0.00      0.00         0\n",
            "          FW       0.00      0.00      0.00         0\n",
            "          NN       0.92      0.89      0.90      2383\n",
            "         RBR       0.50      0.47      0.48        15\n",
            "         PRP       0.97      0.82      0.89       192\n",
            "         PDT       0.00      0.00      0.00         4\n",
            "         VBP       0.91      0.88      0.89       134\n",
            "          DT       0.99      0.87      0.93      1335\n",
            "         VBZ       0.96      0.95      0.95       280\n",
            "         WDT       0.96      0.96      0.96        84\n",
            "       -RRB-       0.10      0.44      0.16        18\n",
            "         JJR       0.87      0.68      0.76        59\n",
            "         WP$       1.00      0.75      0.86         4\n",
            "          RP       0.58      0.79      0.67        33\n",
            "         WRB       1.00      0.71      0.83        24\n",
            "         VBD       0.94      0.93      0.93       634\n",
            "        NNPS       0.00      0.00      0.00        44\n",
            "         POS       0.98      1.00      0.99       152\n",
            "         NNS       0.92      0.90      0.91       941\n",
            "          TO       1.00      1.00      1.00       386\n",
            "         JJS       0.97      0.94      0.95        31\n",
            "       -LRB-       0.26      0.44      0.33        18\n",
            "         SYM       0.00      0.00      0.00         0\n",
            "          VB       0.95      0.96      0.96       403\n",
            "        PRP$       1.00      0.95      0.97        99\n",
            "          CC       1.00      0.92      0.96       366\n",
            "          RB       0.85      0.78      0.82       381\n",
            "          LS       0.00      0.00      0.00         0\n",
            "         VBN       0.91      0.74      0.82       366\n",
            "          IN       0.96      0.93      0.94      1630\n",
            "          WP       1.00      1.00      1.00        20\n",
            "         VBG       0.88      0.83      0.85       221\n",
            "          EX       1.00      0.80      0.89         5\n",
            "         NNP       0.75      0.95      0.84      1504\n",
            "\n",
            "   micro avg       0.89      0.89      0.89     13712\n",
            "   macro avg       0.73      0.69      0.70     13712\n",
            "weighted avg       0.90      0.89      0.90     13712\n",
            "\n",
            "Tags with worst f1 value\n",
            "\tTags:\t\t\t ['UH', 'FW', 'LS', 'PDT', 'SYM', 'NNPS']\n",
            "\t F1:\t\t\t [0. 0. 0. 0. 0. 0.]\n",
            "       Support:\t\t\t [ 0  0  0  4  0 44]\n",
            "Support percentage on total(%):\t [0.    0.    0.    0.029 0.    0.321]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzzmLLG7_Lmr"
      },
      "source": [
        "If we try to check the supports of the tags with lowest f1 score we can see that they have very small or none support. This of course affects the evaluation of our model, and we can see this even looking at the micro average f1 and the weighted average f1 that, giving weights to the score according to support, show greater results.\n",
        "\n",
        "Possible solutions:\n",
        "\n",
        "*  Oversample the tags with smaller supports and undersample the ones with bigger ones\n",
        "*   Change the split of train, val, test in order to get more balanced classes\n",
        "*   Modify the embedding using one single vocabulary, in order to have the same embedding for OOV in train test and split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}